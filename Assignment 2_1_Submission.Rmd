---
title: "STAT443 Forecasting - Assignment 2"
author: "Christopher Risi - Group B - 20239762"
date: "June 7, 2018"
output:
  pdf_document: default
  word_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## 1 Theory
### Problem 1
_Suppose that_ $Y_t$ _is stationary, Gaussian, with zero mean. Let_ $I_t = \{Y_t, Y_{t-1}\}$.  _Show that the optimal linear forecast of_ $Y_{t+2}$ _is_

$$\hat{Y}_{t+2} = \Bigg(\frac{\rho(2)-\rho(3)\rho(1)}{1-\rho^2(1)}\Bigg) Y_t + \Bigg(\frac{\rho(3)-\rho(2)\rho(1)}{1-\rho^2(1)}\Bigg) Y_{t-1}$$
Begin with:
$$\hat{Y}_{t+2} = \alpha + \phi_0Y_t + \phi_1Y_{t-1}$$
but $\alpha = 0$ to ensure that $\hat{Y}_{t+2}$ is unbiased.

The forecasting error $e_{t+2}$ is:
$$e_{t+2} = Y_{t+2} - \hat{Y}_{t+2} = Y_{t+2} - \phi_0Y_t - \phi_1Y_{t-1}$$
Our goal is to minimize $var(e_{t+2})$:
\begin{equation}
\begin{aligned}
S(\phi_0, \phi_1) &= var(e_{t+2}) = E\Big[\Big((Y_{t+2} - \phi_0Y_t - \phi_1Y_{t-1})-E[Y_{t+2} - \phi_0Y_t - \phi_1Y_{t-1}]\Big)^2\Big]\\
S(\phi_0, \phi_1) &= E\Big[(Y_{t+2} - \phi_0Y_t - \phi_1Y_{t-1})^2\Big] \\
S(\phi_0, \phi_1) &= E\big[Y^2_{t+2}\big] - 2\phi_0E\big[Y_{t+2}Y_t\big] - 2\phi_1E\big[Y_{t+2}Y_{t-1}\big] + \phi^2_0E\big[Y^2_{t}\big] + 2\phi_0\phi_1E\big[Y_{t}Y_{t-1}\big] + \phi^2_1E\big[Y^2_{t-1}\big]\\
S(\phi_0, \phi_1) &= \gamma(0) - 2\phi_0\gamma(2) - 2\phi_1\gamma(3) + \phi^2_0\gamma(0) + 2\phi_0\phi_1\gamma(1) + \phi^2_1\gamma(0)\\
\end{aligned}
\end{equation}

Next we take the partial derivatives with respect to $\phi_0$ and $\phi_1$ and set $\frac{\delta S}{\delta \phi_0}$ and $\frac{\delta S}{\delta \phi_1}$ equal to $0$.
\begin{equation}
\begin{aligned}
\frac{\delta S}{\delta \phi_0} = -2\gamma(2) + 2 \phi_0 \gamma(0) + 2\phi_1\gamma(1) &= 0 \\
\phi_0   &= \frac{\gamma(2) - \phi_1\gamma(1)}{\gamma(0)}
\end{aligned}
\end{equation}

and 

\begin{equation}
\begin{aligned}
\frac{\delta S}{\delta \phi_1} = -2\gamma(3) + 2 \phi_0 \gamma(1) + 2\phi_1\gamma(0) &= 0 \\
\phi_1 &= \frac{\gamma(3) - \phi_0 \gamma(1)}{\gamma(0)}
\end{aligned}
\end{equation}

\pagebreak

Using substitution:
\begin{equation}
\begin{aligned}
\phi_0 &= \frac{\gamma(2) - \frac{\gamma(3) - \phi_0 \gamma(1)}{\gamma(0)}\gamma(1)}{\gamma(0)} \\
\phi_0 &= \rho(2) - (\rho(3) - \phi_0 \rho(1)) \rho(1) \\
\phi_0 &= \rho(2) - \rho(3)\rho(1) + \phi_0 \rho(1)^2\\
\phi_0 - \phi_0 \rho(1)^2 &= \rho(2) - \rho(3)\rho(1)  \\
\phi_0(1 -  \rho(1)^2) &= \rho(2) - \rho(3)\rho(1)  \\
\phi_0 &= \frac{\rho(2) - \rho(3)\rho(1)}{1 -  \rho(1)^2}  \\
\end{aligned}
\end{equation}

and then

\begin{equation}
\begin{aligned}
\phi_1 &= \frac{\gamma(3) - \phi_0 \gamma(1)}{\gamma(0)}  \\
\phi_1 &= \rho(3) - \phi_0 \rho(1) \\
\phi_1 &= \rho(3) - \frac{\rho(2) - \rho(3)\rho(1)}{1 -  \rho(1)^2} \rho(1) \\
\phi_1 &= \frac{\rho(3)(1 -  \rho(1)^2)}{1 -  \rho(1)^2} - \frac{\rho(2)\rho(1) - \rho(3)\rho(1)^2}{1 -  \rho(1)^2}  \\
\phi_1 &= \frac{\rho(3)(1 -  \rho(1)^2) - \rho(2)\rho(1) + \rho(3)\rho(1)^2}{1 -  \rho(1)^2}  \\
\phi_1 &= \frac{\rho(3) - \rho(2)\rho(1)}{1 -  \rho(1)^2}  \\
\end{aligned}
\end{equation}

We now need to check that what we have found is in fact a minimum, which can be shown if the determinant of the Hessian Matrix is greater than zero, defined by:

\begin{equation}
\begin{aligned}
det \Bigg(\begin{bmatrix}    
    \frac{\delta^2S}{\delta\phi^2_0} & \frac{\delta^2S}{\delta\phi_0\delta\phi_1}\\
    \frac{\delta^2S}{\delta\phi_1\delta\phi_0} & \frac{\delta^2S}{\delta\phi^2_1}\\
\end{bmatrix}\Bigg) &= \frac{\delta^2 S}{\delta\phi^2_0} \frac{\delta^2 S}{\delta\phi^2_1} - \frac{\delta^2S}{\delta\phi_0\delta\phi_1} \frac{\delta^2S}{\delta\phi_1\delta\phi_0}\\
det \Bigg(\begin{bmatrix}    
    2\gamma(0)      & 2\gamma(1)\\
    2\gamma(1)      & 2\gamma(0)\\
\end{bmatrix}\Bigg) &= 4\gamma(0)^2 - 4\gamma(1)^2\\
\end{aligned}
\end{equation}

and we know that $\gamma(k) < \gamma(0)$ because of $var(Y_t - Y_{t+k}) = \gamma(0) + \gamma(0) - 2\gamma(k) \geq 0 \hspace{0.5cm} \forall k>0$ and we can drop the equality because of the Law of Imperfect Prediction, which implies that:

\begin{equation}
\begin{aligned}
\gamma(0) &>  \gamma(k) \\
\gamma(0)^2 &>  \gamma(k)^2 \\
4\gamma(0)^2  &>  4\gamma(k)^2 \\
4\gamma(0)^2  -  4\gamma(k)^2 &> 0 \\
\end{aligned}
\end{equation}

\pagebreak

Finally we need to show that $e_{t+2}$ is independent of $I_t = \{Y_t, Y_{t-1}\}$ which we can do by checking that the covariance is zero.  This is sufficient because  $e_{t+2}, Y_t, Y_{t-1}$ are Gaussian:

\begin{equation}
\begin{aligned}
cov(e_{t+2}, Y_t) &= E[(e_{t+2} - E[e_{t+2}])(Y_t - E[Y_t])]\\
cov(e_{t+2}, Y_t) &= E[(Y_{t+2} - \phi_0Y_t - \phi_1Y_{t-1})(Y_t)]\\
cov(e_{t+2}, Y_t) &= E[Y_{t+2}Y_t - \phi_0Y_t^2 - \phi_1Y_{t-1}Y_t]\\
cov(e_{t+2}, Y_t) &= E[Y_{t+2}Y_t] - \phi_0E[Y_t^2] - \phi_1E[Y_{t-1}Y_t]\\
cov(e_{t+2}, Y_t) &= \gamma(2) - \phi_0\gamma(0) - \phi_1\gamma(1)\\
cov(e_{t+2}, Y_t) &= \gamma(2) - (\frac{\gamma(2) - \phi_1\gamma(1)}{\gamma(0)})\gamma(0) - \phi_1\gamma(1)\\
cov(e_{t+2}, Y_t) &= \gamma(2) - \gamma(2) + \phi_1\gamma(1) - \phi_1\gamma(1) = 0\\
\end{aligned}
\end{equation}

and

\begin{equation}
\begin{aligned}
cov(e_{t+2}, Y_{t-1}) &= E[(e_{t+2} - E[e_{t+2}])(Y_{t-1} - E[Y_{t-1}])]\\
cov(e_{t+2}, Y_{t-1}) &= E[(Y_{t+2} - \phi_0Y_t - \phi_1Y_{t-1})(Y_{t-1})]\\
cov(e_{t+2}, Y_{t-1}) &= E[Y_{t+2}Y_{t-1} - \phi_0Y_t Y_{t-1} - \phi_1Y_{t-1}^2]\\
cov(e_{t+2}, Y_{t-1}) &= E[Y_{t+2}Y_{t-1}] - \phi_0E[Y_t Y_{t-1}] - \phi_1E[Y_{t-1}^2]\\
cov(e_{t+2}, Y_{t-1}) &= \gamma(3) - \phi_0\gamma(1) - \phi_1\gamma(0)\\
cov(e_{t+2}, Y_{t-1}) &= \gamma(3) -  \phi_0\gamma(1) - (\frac{\gamma(3) - \phi_0 \gamma(1)}{\gamma(0)})\gamma(0)\\ 
cov(e_{t+2}, Y_{t-1}) &= \gamma(3) - \phi_0\gamma(1) - \gamma(3) + \phi_0\gamma(1) = 0\\
\end{aligned}
\end{equation}

Both covariances equal 0, therefore $e_{t+2}$ is independent of $I_t = \{Y_t, Y_{t-1}\}$.

\pagebreak

### Problem 2
_Suppose_ $Y_t$ _is Gausssian.  Let_ $I_t = \{Y_{t-1}, Y_{t-2} \} = \{0.02, 0.01\}$, $\gamma(0) = 0.0016$, $\rho(1)= \frac{2}{3}$, _and_ $\rho(2) = \frac{1}{3}$. _Construct a 95% confidence interval for_ $Y_t$ _and explain its meaning._

From Theorem 44 (Sampson 2001), if the stationary stochastic process is Gaussian (normally distributed) a 95% confidence interval for $Y_{t+k}$ is:

$$E_t\big[Y_{t+k}\big] \pm 1.96 \sqrt{var_t(Y_{t+k})} $$
Also from tutorial

$$E_t\big[Y_t \big] = \phi_{21}Y_{t-1} + \phi_{22}Y_{t-2}$$
To solve for $\phi_{21}$ and $\phi_{22}$ we minimize $var{e_t}$ and solve:

\begin{equation}
\begin{aligned}
var(e_t) &= var(Y_t - \phi_{21} Y_{t-1} - \phi_{22}Y_{t-2}) \\
var(e_t) &= E\bigg[\Big((Y_t - \phi_{21} Y_{t-1} - \phi_{22}Y_{t-2}) -E\big[(Y_t - \phi_{21} Y_{t-1} - \phi_{22}Y_{t-2})\big]\Big)^2 \bigg]\\
var(e_t) &= E\bigg[\Big((Y_t - \phi_{21} Y_{t-1} - \phi_{22}Y_{t-2})\Big)^2 \bigg]\\
var(e_t) &= E\big[Y^2_t\big] - 2 \phi_{21} E\big[Y_{t}Y_{t-1}\big] - 2 \phi_{22}E\big[Y_{t}Y_{t-2}\big] + \phi^2_{21} E\big[Y^2_{t-1}\big] - 2 \phi_{21}\phi_{22} E\big[Y_{t-1}Y_{t-2}\big] + \phi^2_{22} E\big[Y^2_{t-2}\big]\\
var(e_t) &= \gamma(0) - 2 \phi_{21} \gamma(1) - 2 \phi_{22}\gamma(2) + \phi^2_{21} \gamma(0) - 2 \phi_{21}\phi_{22} \gamma(1) + \phi^2_{22}\gamma(0)\\
\end{aligned}
\end{equation}

The we differentiate with respect to $\phi$.

\begin{equation}
\begin{aligned}
\frac{\delta S}{\delta \phi_{21}} = -2\gamma(1) + 2 \phi_{21} \gamma(0) + 2\phi_{22}\gamma(1) &= 0 \\
\phi_{21} &= \frac{\gamma(1) - \phi_{22}\gamma(1)}{\gamma(0)} = \rho(1) - \phi_{22} \rho(1)
\end{aligned}
\end{equation}

and 

\begin{equation}
\begin{aligned}
\frac{\delta S}{\delta \phi_{22}} = -2\gamma(2) + 2 \phi_{21} \gamma(1) + 2\phi_{22}\gamma(0) &= 0 \\
\phi_{21} &= \frac{\gamma(2) - \phi_{21}\gamma(1)}{\gamma(0)} = \rho(2) - \phi_{21} \rho(1)
\end{aligned}
\end{equation}

Next we use substitution:
\begin{equation}
\begin{aligned}
\phi_{21} &=
\rho(1) - (\rho(2) - \phi_{21} \rho(1)) \rho(1)\\
\phi_{21} &= \frac{\rho(1) - \rho(1)\rho(2)}{1-\rho(1)^2}\\
\phi_{21} &= \frac{\frac{2}{3} - \frac{2}{3}\frac{1}{3}}{1-(\frac{2}{3})^2}\\
\phi_{21} &= \frac{\frac{6}{9} - \frac{2}{9}}{\frac{5}{9}} = \frac{4}{5}\\
\end{aligned}
\end{equation}

and

\begin{equation}
\begin{aligned}
\phi_{21} &= \frac{1}{3} - \frac{4}{5}\frac{2}{3} = -\frac{3}{15}\\
\end{aligned}
\end{equation}

$$\therefore E_t\big[Y_t \big] = \phi_{21}Y_{t-1} + \phi_{22}Y_{t-2} = (0.8)(0.02)+(-0.2)(0.01) = 0.014$$
and

\begin{equation}
\begin{aligned}
var(Y_t | I_t) &= \gamma(0)(1 - r^T_2 R^{-1}_2 r_2)\\
var(Y_t | I_t) &= 0.0016(1 - \begin{bmatrix} \frac{2}{3} & \frac{1}{3}\\ \end{bmatrix} \begin{bmatrix} 1 & \frac{2}{3}\\ \frac{2}{3} & 1\\ \end{bmatrix}^{-1} \begin{bmatrix} \frac{2}{3} \\ \frac{1}{3} \end{bmatrix})\\
var(Y_t | I_t) &= 0.0016(1 - \begin{bmatrix} \frac{2}{3} & \frac{1}{3}\\ \end{bmatrix} \begin{bmatrix} 1.8 & -1.2 \\ -1.2 & 1.8\\ \end{bmatrix} \begin{bmatrix} \frac{2}{3} \\ \frac{1}{3} \end{bmatrix})\\
var(Y_t | I_t) &= 0.0016(1 - \begin{bmatrix} \frac{2}{3} & \frac{1}{3}\\ \end{bmatrix} \begin{bmatrix} 0.8 \\ -0.2 \end{bmatrix})\\
var(Y_t | I_t) &= 0.0016(1 - \frac{7}{15})\\
var(Y_t | I_t) &= 0.0008533333\\
\end{aligned}
\end{equation}

Finally:

\begin{equation}
\begin{aligned}
0.95 &\approx Pr\Big(E[Y_t] - 1.96 \sqrt{var(Y_t)} < Y_t < E[Y_t] + 1.96 \sqrt{var(Y_t)}\Big) \\
0.95 &\approx Pr\Big(0.014 1.96 \sqrt{0.0008533333} < Y_t < 0.014 + 1.96 \sqrt{0.0008533333}\Big) \\
0.95 &\approx Pr\Big(-0.04326 < Y_t < 0.07126 \Big) \\
\end{aligned}
\end{equation}

We are 95% certain that the interval $(-0.04326, 0.07126)$ contains the true unknown $Y_t | Y_{t-1}, Y_{t-2}$.

\pagebreak

## 2 Practicum

### Problem 1
Use a random number generator to generate $\theta$ from a uniform distribution with lower bound 0.5 and upper bound 0.9. Generate $\sigma$ from a uniform distribution with lower bound 0.02 and upper bound 0.08. Suppose that $Y_t$ follows the $MA(1)$ representation:

$$Y_t = \epsilon_t + \theta \epsilon_{t-1}, \hspace{0.5cm} \epsilon_t \hspace{0.1cm} \sim \hspace{0.1cm} i.i.d. N[0,\sigma^2]$$

Calculate $\rho(k)$, $\phi_{kk}$ for $k = 1,2,3$. For $k=3$, you can use the matrix inversion and multiplication commands in your computer package.  Calculate the optimal forecast rule

$$E[Y_t|Y_{t-1}, Y_{t-2}, Y_{t-3}]$$
and
$$var[Y_t|Y_{t-1}, Y_{t-2}, Y_{t-3}]$$
Using a random number generator simulate $T = 200$ observations of $Y_t = \epsilon_t + \theta \epsilon_{t-1}$ and present a graph of $Y_t$.  Using your simulated data find $\hat{\rho}(k)$, $\hat{\phi}_{kk}$ for $k = 1,2,3$ by running the appropriate regressions.
Why do $\rho(k)$, $\phi_{kk}$ and $\hat{\rho}(k)$, and $\hat{\phi}_{kk}$ differ?

#### Generate $\theta$ and $\sigma$
```{r 1_a, eval=TRUE, echo=FALSE, warning=FALSE, message=FALSE}
set.seed(271) 
theta <- runif(1, 0.5, 0.9)
sigma <- runif(1, 0.02, 0.08)
theta
sigma
```

#### Calculate $\rho(k)$, $\phi_{kk}$ for $k = 1,2,3$


\[\rho(k) = \left\{
  \begin{array}{lr}
    \frac{\theta}{1+\theta^2} &  k = 1\\
    0 &  k > 1
  \end{array}
\right.
\]

\[\rho(k) = \left\{
  \begin{array}{lr}
    \frac{0.6017529}{1+0.6017529^2} \hspace{0.2cm} = 0.4417811 &  k = 1\\
    0 &  k > 1
  \end{array}
\right.
\]

##### k = 1
\begin{equation}
\begin{aligned}
\hat{Y}_t &= \phi_{11} Y_{t-1} \\
\\
\phi_{11} &= R_1 r_1 = [1] \rho(1) = 0.4417811 \\
\end{aligned}
\end{equation}
```{r CalculateRho(1), eval=TRUE, echo=FALSE, warning=FALSE, message=FALSE}
rho1 <- theta/(1 + theta^2)
rho1
```

##### k = 2

\begin{equation}
\begin{aligned}
\hat{Y}_t &= \phi_{21} Y_{t-1} + \phi_{22} Y_{t-2} \\
\\
\hat{\Phi} &= \begin{bmatrix} \phi_{21} \\ \phi_{22} \\ \end{bmatrix} = R^{-1}_2 r_2 =  \begin{bmatrix} 1 & \rho(1) \\ \rho(1) & 1 \\ \end{bmatrix}^{-1}\begin{bmatrix} \rho(1) \\ \rho(2) \\ \end{bmatrix}\\
\hat{\Phi} &= \begin{bmatrix} \phi_{21} \\ \phi_{22} \\ \end{bmatrix} =  \begin{bmatrix} 1 & 0.4417811 \\ 0.4417811 & 1 \\ \end{bmatrix}^{-1}\begin{bmatrix} 0.4417811 \\ 0 \\ \end{bmatrix} = \begin{bmatrix} 0.5489126 \\ -0.2424992 \\ \end{bmatrix}\\
\end{aligned}
\end{equation}

```{r CalculatePhi21Phi22, eval=TRUE, echo=FALSE, warning=FALSE, message=FALSE}
library(matlib)
R_2 <- matrix(c(1, rho1,rho1, 1), ncol = 2, nrow = 2)
r_2 <- matrix(c(rho1, 0), ncol = 1, nrow = 2)
R_2
r_2
Phi_2 <- inv(R_2) %*% r_2
Phi_2

```

##### k = 3

\begin{equation}
\begin{aligned}
\hat{Y}_t &= \phi_{31} Y_{t-1} + \phi_{32} Y_{t-2} + \phi_{33} Y_{t-3}\\
\\
\hat{\Phi} &= \begin{bmatrix} \phi_{31} \\ \phi_{32} \\ \phi_{33}\\ \end{bmatrix} = R^{-1}_3 r_3 =  \begin{bmatrix} 1 & \rho(1) & \rho(2) \\ \rho(1) & 1 & \rho(1)\\ \rho(2) & \rho(1) & 1 \end{bmatrix}^{-1}\begin{bmatrix} \rho(1) \\ \rho(2) \\ \rho(3) \\ \end{bmatrix}\\
\hat{\Phi} &= \begin{bmatrix} \phi_{31} \\ \phi_{32} \\ \phi_{33}\\ \end{bmatrix} = R^{-1}_3 r_3 =  \begin{bmatrix} 1 & 0.4417811 & 0 \\ 0.4417811 & 1 & 0.4417811 \\ 0 & 0.4417811 & 1 \end{bmatrix}^{-1}\begin{bmatrix} 0.4417811 \\ 0 \\ 0 \\ \end{bmatrix} = \begin{bmatrix} 0.5832087 \\ -0.3201306 \\ 0.1414277 \\ \end{bmatrix}\\
\end{aligned}
\end{equation}

```{r CalculatePhi31Phi32Phi33, eval=TRUE, echo=FALSE, warning=FALSE, message=FALSE}
R_3<- matrix(c(1, rho1, 0,  rho1, 1, rho1, 0, rho1, 1), ncol = 3, nrow = 3)
r_3 <- matrix(c(rho1, 0, 0), ncol = 1, nrow = 3)
R_3
r_3
Phi_3 <- inv(R_3) %*% r_3
Phi_3

```

So $\phi_{11} = 0.4417811, \phi_{22} = -0.2424992, \phi_{33} = 0.1414277$

\pagebreak

#### Calculate the optimal forecast rule:

$$E[Y_t | Y_{t-1}, Y_{t-2}, Y_{t-3}] = \hat{Y}_t = \phi_{31}Y_{t-1} + \phi_{32}Y_{t-2} + \phi_{33}Y_{t-3}$$
$$E[Y_t | Y_{t-1}, Y_{t-2}, Y_{t-3}] = 0.583Y_{t-1} -0.320Y_{t-2} + 0.141Y_{t-3}$$
and


\begin{equation}
\begin{aligned}
var(Y_t | Y_{t-1}, Y_{t-2}, Y_{t-3}) &= \gamma(0)(1 - r^T_3 R^{-1}_3 r_3) \\
var(Y_t | Y_{t-1}, Y_{t-2}, Y_{t-3}) &= \sigma^2(1+\theta^2)(1 - r^T_3 R^{-1}_3 r_3)\\
var(Y_t | Y_{t-1}, Y_{t-2}, Y_{t-3}) &= \sigma^2(1+\theta^2)(1 -\begin{bmatrix} 0.4417811 & 0 & 0 \\ \end{bmatrix} \begin{bmatrix} 0.5832087 \\ -0.3201306 \\ 0.1414277 \\ \end{bmatrix})\\
var(Y_t | Y_{t-1}, Y_{t-2}, Y_{t-3}) &= (0.05238621^2)(1+ 0.6017529^2)(1 - 0.2576506) = 0.002774939
\end{aligned}
\end{equation}

```{r 1_CalculateOptimalVariance, eval=TRUE, echo=FALSE, warning=FALSE, message=FALSE}
(sigma^2)*(1+theta^2)*(1 - t(r_3) %*% Phi_3)
```

#### Simulate $T = 200$ Observations of $Y_t = \epsilon_t + \theta \epsilon_{t-1}$
```{r 1_b, eval=TRUE, echo=FALSE, warning=FALSE, message=FALSE}
epsilon_t <- rnorm(201, mean = 0, sd = sigma)
Y_t <- epsilon_t[-1] + theta*epsilon_t[-length(epsilon_t)]
head(epsilon_t)
head(Y_t)
```

#### Present a graph of $Y_t$
```{r 1_plot, eval=TRUE, echo=FALSE, warning=FALSE, message=FALSE, fig.height = 4,}
library(ggplot2)
qplot(seq_along(Y_t), Y_t, geom="line") + xlab("Index") + ggtitle("Graph of Y_t")
```


\pagebreak

#### 5 different models using two equations the $\hat{Y}_t = \rho(k) Y_{t-k}$ and $\hat{Y}_t = \phi_{k1}Y_{t-1} + \phi_{k2}Y_{t-2} + \phi_{k3}Y_{t-3} + ...$

##### Estimation of $\hat{\rho}(1)$ and $\hat{\phi}_{11}$ using formula $\hat{Y}_t = \rho(1) Y_{t-1}$
```{r model_rho1, eval=TRUE, echo=FALSE, warning=FALSE, message=FALSE}
model_rho1 <- lm(Y_t[-1] ~ Y_t[-length(Y_t)] -1)
summary(model_rho1)
rho_1 <- as.numeric(model_rho1$coef)
rho_1
```
Our estimate $\hat{\rho}(1) = \hat{\phi}_{11} = 0.35170$ is fairly close to our theoretical value of $\rho(1) = \phi_{11} = 0.4417811$

##### Estimation of $\hat{\rho}(2)$ using formula $\hat{Y}_t = \rho(2) Y_{t-2}$
```{r model_rho2, eval=TRUE, echo=FALSE, warning=FALSE, message=FALSE}
model_rho2 <- lm(Y_t[-c(1,2)] ~ Y_t[-((length(Y_t)-1):length(Y_t))] -1)
summary(model_rho2)
rho_2 <- as.numeric(model_rho2$coef)
rho_2
```
This estimate $\hat{\rho}(2)$ is close to zero which makes sense for an MA(1). 

##### Estimation of $\hat{\rho}(3)$ using formula $\hat{Y}_t = \rho(3) Y_{t-3}$
```{r model_rho3, eval=TRUE, echo=FALSE, warning=FALSE, message=FALSE}
model_rho3 <- lm(Y_t[-c(1,2,3)] ~ Y_t[-((length(Y_t)-2):length(Y_t))] -1)
summary(model_rho3)
rho_3 <- as.numeric(model_rho3$coef)
rho_3
```
This estimate $\hat{\rho}(3)$ is close to zero which makes sense for an MA(1). 

##### Estimation of $\hat{\phi}_{21}$ and $\hat{\phi}_{22}$ using formula $\hat{Y}_t = \phi_{21} Y_{t-1} + \phi_{22} Y_{t-2}$
```{r model_Phi_2, eval=TRUE, echo=FALSE, warning=FALSE, message=FALSE}
model_Phi_2 <- lm(Y_t[-c(1,2)] ~ Y_t[-c(1,length(Y_t))] + Y_t[-((length(Y_t)-1):length(Y_t))] -1)
summary(model_Phi_2)
phi_21 <- as.numeric(model_Phi_2$coef)[1]
phi_22 <- as.numeric(model_Phi_2$coef)[2]
phi_21
phi_22
```
Our estimates are fairly close to the theoretical values of 0.5489 and -0.2425.

##### Estimation of $\hat{\phi}_{31}$, $\hat{\phi}_{31}$, and  $\hat{\phi}_{33}$ using formula $\hat{Y}_t = \phi_{31} Y_{t-1} + \phi_{32} Y_{t-2}+ \phi_{33} Y_{t-3}$
```{r model_Phi_3, eval=TRUE, echo=FALSE, warning=FALSE, message=FALSE}
model_Phi_3 <- lm(Y_t[-c(1,2,3)] ~ Y_t[-c(1,2,length(Y_t))] + Y_t[-c(1,(length(Y_t)-1):length(Y_t))] + Y_t[-((length(Y_t)-2):length(Y_t))] -1)
summary(model_Phi_3)
phi_31 <- as.numeric(model_Phi_3$coef)[1]
phi_32 <- as.numeric(model_Phi_3$coef)[2]
phi_33 <- as.numeric(model_Phi_3$coef)[3]
phi_31
phi_32
phi_33
```
Our estimates are fairly close to the theoretical values of 0.583, -0.320, and 0.141.

Obviously all of our values are just estimates and will not be expected to match the theoretical values exactly.  As T increases we would expect our estimate to become more accurate.

In summary:

* $\hat{\rho}(1) =  0.3517026$ 
* $\hat{\rho}(2) =  -0.04524958$
* $\hat{\rho}(3) =  0.03959823$
* $\hat{\phi}_{11} = 0.3517026$
* $\hat{\phi}_{22} = -0.1950678$
* $\hat{\phi}_{33} = 0.1485742$

\pagebreak

### Problem 2
Using a standard normal random number generator, simulate $T = 200$ observations of the $AR(1)$ representation for $Y_t$:

$$Y_t = \phi Y_{t-1} + \epsilon_t, \hspace{0.5cm} \epsilon_t \hspace{0.1cm} \sim \hspace{0.1cm} i.i.d N[0,\sigma^2],$$

where $\sigma = 0.02$ for $\phi = -0.75$, $\phi = 0$, $\phi = 0.75$, $\phi = 0.95$, and $\phi = 1$. Plot all five graphs with 95% confidence bands.


```{r 2_a, eval=TRUE, echo=FALSE, warning=FALSE, message=FALSE, fig.height = 4, fig.width = 7, fig.align = "center"}
library(ggplot2)
sigma <- 0.02
epsilon_t <- rnorm(200, mean = 0, sd = 0.02)

### For Phi = -0.75
Y_n075 <- rep(0, 200)
for(i in 1:199){
  Y_n075[i+1] <- Y_n075[i]*(-0.75) + epsilon_t[i]
  i = i + 1
} 
sqrt(var(diff(Y_n075)))
lwd_Yt = mean(Y_n075) - 1.96*sigma/sqrt(1-(-0.75)^2) 
upr_Yt = mean(Y_n075) + 1.96*sigma/sqrt(1-(-0.75)^2) 
qplot(seq_along(Y_n075), Y_n075, geom = "line") +
  geom_hline(yintercept=lwd_Yt) + 
  geom_hline(yintercept=upr_Yt) + ggtitle("Phi = -0.75") + xlab("Index") + ylab("Y_t")

### For Phi = 0
Y_0 <- rep(0, 200)
for(i in 1:199){
  Y_0[i+1] <- Y_0[i]*(0) +  epsilon_t[i]
  i = i + 1
} 
lwd_Yt = mean(Y_0) - 1.96*sigma/sqrt(1-(0)^2) 
upr_Yt = mean(Y_0) + 1.96*sigma/sqrt(1-(0)^2) 
qplot(seq_along(Y_0), Y_0, geom = "line") + 
  geom_hline(yintercept=lwd_Yt) + 
  geom_hline(yintercept=upr_Yt) + ggtitle("Phi = 0") + xlab("Index") + ylab("Y_t")

### For Phi = 0.75
Y_075 <- rep(0, 200)
for(i in 1:199){
  Y_075[i+1] <- Y_075[i]*(0.75) + epsilon_t[i]
  i = i + 1
}  
lwd_Yt = mean(Y_075) - 1.96*sigma/sqrt(1-(0.75)^2) 
upr_Yt = mean(Y_075) + 1.96*sigma/sqrt(1-(0.75)^2) 
qplot(seq_along(Y_075), Y_075, geom = "line") +
  geom_hline(yintercept=lwd_Yt) + 
  geom_hline(yintercept=upr_Yt) + ggtitle("Phi = 0.75") + xlab("Index") + ylab("Y_t")

### For Phi = 0.95
Y_095 <- rep(0, 200)
for(i in 1:199){
  Y_095[i+1] <- Y_095[i]*(0.95) + epsilon_t[i]
  i = i + 1
} 
lwd_Yt = mean(Y_095) - 1.96*sigma/sqrt(1-(0.95)^2) 
upr_Yt = mean(Y_095) + 1.96*sigma/sqrt(1-(0.95)^2) 
qplot(seq_along(Y_095), Y_095, geom = "line") +
  geom_hline(yintercept=lwd_Yt) + 
  geom_hline(yintercept=upr_Yt) + ggtitle("Phi = 0.95") + xlab("Index") + ylab("Y_t")
```

```{r 2_phi1, eval=TRUE, echo=FALSE, warning=FALSE, message=FALSE, fig.height = 4, fig.width = 7, fig.align = "center"}
### For Phi = 1
Y_1 <- rep(0, 200)
tSeq <- 1:200
for(i in 1:199){
  Y_1[i+1] <- Y_1[i]*(1) + epsilon_t[i]
  i = i + 1
} 
lwd_Yt = data.frame(lwd_Yt = mean(Y_1) - 1.96*sqrt(var(diff(Y_1)))*sqrt(tSeq))
upr_Yt = data.frame(upr_Yt = mean(Y_1) + 1.96*sqrt(var(diff(Y_1)))*sqrt(tSeq))
Y_1 <- data.frame(Y_1)
ggplot(Y_1, aes(x = seq_along(Y_1), y = Y_1)) + 
  geom_line() +
  geom_line(aes(x = 1:200 , y = lwd_Yt)) + 
  geom_line(aes(x = 1:200 , y = upr_Yt)) + ggtitle("Phi = 1") + xlab("Index") + ylab("Y_t")


```

\pagebreak

## Appendix Code
```{r Appendix, eval=FALSE}
#### Generate $\theta$ and $\sigma$
set.seed(271) 
theta <- runif(1, 0.5, 0.9)
sigma <- runif(1, 0.02, 0.08)
theta
sigma
#### Calculate $\rho(k)$, $\phi_{kk}$ for $k = 1,2,3$

##### k = 1

rho1 <- theta/(1 + theta^2)
rho1

##### k = 2

library(matlib)
R_2 <- matrix(c(1, rho1,rho1, 1), ncol = 2, nrow = 2)
r_2 <- matrix(c(rho1, 0), ncol = 1, nrow = 2)
R_2
r_2
Phi_2 <- inv(R_2) %*% r_2
Phi_2


##### k = 3
R_3<- matrix(c(1, rho1, 0,  rho1, 1, rho1, 0, rho1, 1), ncol = 3, nrow = 3)
r_3 <- matrix(c(rho1, 0, 0), ncol = 1, nrow = 3)
R_3
r_3
Phi_3 <- inv(R_3) %*% r_3
Phi_3

#### Calculate the optimal forecast rule:
sigma
theta
(sigma^2)*(1+theta^2)*(t(r_3) %*% Phi_3)

#### Simulate $T = 200$ Observations of $Y_t = \epsilon_t + \theta \epsilon_{t-1}$
epsilon_t <- rnorm(201, mean = 0, sd = sigma)
Y_t <- epsilon_t[-1] + theta*epsilon_t[-length(epsilon_t)]

#### Present a graph of $Y_t$
library(ggplot2)
qplot(seq_along(Y_t), Y_t, geom="line") + xlab("Index") + ggtitle("Graph of Y_t")

##### Estimation of $\hat{\rho}(1)$ and $\hat{\phi}_{11}$ using formula $\hat{Y}_t = \rho(1) Y_{t-1}$
model_rho1 <- lm(Y_t[-1] ~ Y_t[-length(Y_t)] -1)
summary(model_rho1)
rho_1 <- as.numeric(model_rho1$coef)
rho_1

##### Estimation of $\hat{\rho}(2)$ using formula $\hat{Y}_t = \rho(2) Y_{t-2}$
model_rho2 <- lm(Y_t[-c(1,2)] ~ Y_t[-((length(Y_t)-1):length(Y_t))] -1)
summary(model_rho2)
rho_2 <- as.numeric(model_rho2$coef)
rho_2

##### Estimation of $\hat{\rho}(3)$ using formula $\hat{Y}_t = \rho(3) Y_{t-3}$
model_rho3 <- lm(Y_t[-c(1,2,3)] ~ Y_t[-((length(Y_t)-2):length(Y_t))] -1)
summary(model_rho3)
rho_3 <- as.numeric(model_rho3$coef)
rho_3

##### Estimation of $\hat{\phi}_{21}$ and $\hat{\phi}_{22}$ using formula $\hat{Y}_t = \phi_{21} Y_{t-1} + \phi_{22} Y_{t-2}$
model_Phi_2 <- lm(Y_t[-c(1,2)] ~ Y_t[-c(1,length(Y_t))] + Y_t[-((length(Y_t)-1):length(Y_t))] -1)
summary(model_Phi_2)
phi_21 <- as.numeric(model_Phi_2$coef)[1]
phi_22 <- as.numeric(model_Phi_2$coef)[2]
phi_21
phi_22

##### Estimation of $\hat{\phi}_{31}$, $\hat{\phi}_{31}$, and  $\hat{\phi}_{33}$ using formula $\hat{Y}_t = \phi_{31} Y_{t-1} + \phi_{32} Y_{t-2}+ \phi_{33} Y_{t-3}$
model_Phi_3 <- lm(Y_t[-c(1,2,3)] ~ Y_t[-c(1,2,length(Y_t))] + Y_t[-c(1,(length(Y_t)-1):length(Y_t))] + Y_t[-((length(Y_t)-2):length(Y_t))] -1)
summary(model_Phi_3)
phi_31 <- as.numeric(model_Phi_3$coef)[1]
phi_32 <- as.numeric(model_Phi_3$coef)[2]
phi_33 <- as.numeric(model_Phi_3$coef)[3]
phi_31
phi_32
phi_33

### Problem 2
library(ggplot2)
sigma <- 0.02
epsilon_t <- rnorm(200, mean = 0, sd = 0.02)

### For Phi = -0.75
Y_n075 <- rep(0, 200)
for(i in 1:199){
  Y_n075[i+1] <- Y_n075[i]*(-0.75) + epsilon_t[i]
  i = i + 1
} 
sqrt(var(diff(Y_n075)))
lwd_Yt = mean(Y_n075) - 1.96*sigma/sqrt(1-(-0.75)^2) 
upr_Yt = mean(Y_n075) + 1.96*sigma/sqrt(1-(-0.75)^2) 
qplot(seq_along(Y_n075), Y_n075, geom = "line") +
  geom_hline(yintercept=lwd_Yt) + 
  geom_hline(yintercept=upr_Yt) + ggtitle("Phi = -0.75") + xlab("Index") + ylab("Y_t")

### For Phi = 0
Y_0 <- rep(0, 200)
for(i in 1:199){
  Y_0[i+1] <- Y_0[i]*(0) +  epsilon_t[i]
  i = i + 1
} 
lwd_Yt = mean(Y_0) - 1.96*sigma/sqrt(1-(0)^2) 
upr_Yt = mean(Y_0) + 1.96*sigma/sqrt(1-(0)^2) 
qplot(seq_along(Y_0), Y_0, geom = "line") + 
  geom_hline(yintercept=lwd_Yt) + 
  geom_hline(yintercept=upr_Yt) + ggtitle("Phi = 0") + xlab("Index") + ylab("Y_t")

### For Phi = 0.75
Y_075 <- rep(0, 200)
for(i in 1:199){
  Y_075[i+1] <- Y_075[i]*(0.75) + epsilon_t[i]
  i = i + 1
}  
lwd_Yt = mean(Y_075) - 1.96*sigma/sqrt(1-(0.75)^2) 
upr_Yt = mean(Y_075) + 1.96*sigma/sqrt(1-(0.75)^2) 
qplot(seq_along(Y_075), Y_075, geom = "line") +
  geom_hline(yintercept=lwd_Yt) + 
  geom_hline(yintercept=upr_Yt) + ggtitle("Phi = 0.75") + xlab("Index") + ylab("Y_t")

### For Phi = 0.95
Y_095 <- rep(0, 200)
for(i in 1:199){
  Y_095[i+1] <- Y_095[i]*(0.95) + epsilon_t[i]
  i = i + 1
} 
lwd_Yt = mean(Y_095) - 1.96*sigma/sqrt(1-(0.95)^2) 
upr_Yt = mean(Y_095) + 1.96*sigma/sqrt(1-(0.95)^2) 
qplot(seq_along(Y_095), Y_095, geom = "line") +
  geom_hline(yintercept=lwd_Yt) + 
  geom_hline(yintercept=upr_Yt) + ggtitle("Phi = 0.95") + xlab("Index") + ylab("Y_t")

### For Phi = 1
Y_1 <- rep(0, 200)
tSeq <- 1:200
for(i in 1:199){
  Y_1[i+1] <- Y_1[i]*(1) + epsilon_t[i]
  i = i + 1
} 
lwd_Yt = data.frame(lwd_Yt = mean(Y_1) - 1.96*sqrt(var(diff(Y_1)))*sqrt(tSeq))
upr_Yt = data.frame(upr_Yt = mean(Y_1) + 1.96*sqrt(var(diff(Y_1)))*sqrt(tSeq))
Y_1 <- data.frame(Y_1)
ggplot(Y_1, aes(x = seq_along(Y_1), y = Y_1)) + 
  geom_line() +
  geom_line(aes(x = 1:200 , y = lwd_Yt)) + 
  geom_line(aes(x = 1:200 , y = upr_Yt)) + ggtitle("Phi = 1") + xlab("Index") + ylab("Y_t")

```