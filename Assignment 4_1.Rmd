---
title: "STAT443 Forecasting - Assignment 4"
author: "Christopher Risi - Group B - 20239762"
date: "July 17, 2018"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## 1 Theory

### Problem 1

_Consider the_ $MA(2)$ _representation:_


$$Y_t = \epsilon_t + \theta_1 \epsilon_{t-1} + \theta_2 \epsilon_{t-2}$$


_Let the estimated results be as follows:_

$$\hat Y_t = \epsilon_t + 0.5 \epsilon_{t-1} - 0.2 \epsilon_{t-2},$$

$$RSS = 0.375, \hspace{0.25cm} n = 150$$

_Answer the following questions:_


_1. Show that the variance of _ $Y_t$ _is approximately_ $0.003225$.

\begin{equation}
\begin{aligned}
var(Y_t) &= var(\epsilon_t + \theta_1 \epsilon_{t-1} + \theta_2 \epsilon_{t-2}) \\
var(Y_t) &= var(\epsilon_t) + \theta_1^2 var(\epsilon_{t-1}) + \theta_2^2 var(\epsilon_{t-2}) \\
var(Y_t) &= \sigma^2(1 + \theta_1^2 + \theta_2^2) \\
var(Y_t) &= (0.375/150)(1 + 0.5^2 + (-0.2)^2) = 0.003225 \\
\end{aligned}
\end{equation}


_2. Derive analytically an expression for the autocovariance function_ $\gamma(k)$ _pertaining to the_ $MA(2)$ _representation_ $Y_t = \epsilon_t + \theta_1 \epsilon_{t-1} + \theta_2 \epsilon_{t-2}$.

\begin{equation}
\begin{aligned}
\gamma(0) &= \sigma^2(1 + \theta_1^2 + \theta_2^2) \\
\end{aligned}
\end{equation}

\begin{equation}
\begin{aligned}
\gamma(1) &= E[Y_t Y_{t-1}] = E[(\epsilon_t + \theta_1 \epsilon_{t-1} + \theta_2 \epsilon_{t-2})(\epsilon_{t-1} + \theta_1 \epsilon_{t-2} + \theta_2 \epsilon_{t-3})] \\
\gamma(1) &= E[\theta_1 \epsilon_{t-1}^2 + \theta_1 \theta_2 \epsilon_{t-2}^2] \\
\gamma(1) &= \theta_1 E[\epsilon_{t-1}^2] + \theta_1 \theta_2 E[\epsilon_{t-2}^2] \\
\gamma(1) &= \sigma^2 (\theta_1 + \theta_1 \theta_2) \\
\end{aligned}
\end{equation}

\begin{equation}
\begin{aligned}
\gamma(2) &= E[Y_t Y_{t-2}] = E[(\epsilon_t + \theta_1 \epsilon_{t-1} + \theta_2 \epsilon_{t-2})(\epsilon_{t-2} + \theta_1 \epsilon_{t-3} + \theta_2 \epsilon_{t-4})] \\
\gamma(2) &= E[\theta_2 \epsilon_{t-2}^2] \\
\gamma(2) &= \sigma^2 \theta_2 \\
\end{aligned}
\end{equation}

\begin{equation}
\begin{aligned}
\gamma(k) &= 0 \hspace{0.5cm} for \hspace{0.1cm} k > 2
\end{aligned}
\end{equation}

\pagebreak

_3. Suppose that the forecasst errors_

$$\epsilon = Y_t - E_{t-1}[Y_t] = 0.04$$

_and_

$$ \epsilon_{t-1} = Y_{t-1} - E_{t-2}[Y_{t-1}] = -0.03$$
_Construct a 95% confidence interval for your forecast of_ $Y_{t+1}$.

Begin by constructing the expectation:
\begin{equation}
\begin{aligned}
E[Y_{t+1}] &= E[\epsilon_{t+1} + 0.5 \epsilon_{t} - 0.2 \epsilon_{t-1}] \\
E[Y_{t+1}] &= E[\epsilon_{t+1}] + 0.5 E[\epsilon_{t}] - 0.2 E[\epsilon_{t-1}] \\
E[Y_{t+1}] &= 0 + 0.5 (0.04) - 0.2 (-0.03) = 0.026  \\
\end{aligned}
\end{equation}

Then calculating the variance: 
\begin{equation}
\begin{aligned}
var[Y_{t+1}] &= var[\epsilon_{t+1} + 0.5 \epsilon_{t} - 0.2 \epsilon_{t-1}] \\
var[Y_{t+1}] &= var[\epsilon_{t+1}] + 0.5^2 var[\epsilon_{t}] - 0.2^2 var[\epsilon_{t-1}] + zero cross cov.  \\
var[Y_{t+1}] &= \sigma^2 + 0.5^2 (0) - 0.2^2 (0) = (0.375/150) = 0.0025\\
\end{aligned}
\end{equation}

Finally,

\begin{equation}
\begin{aligned}
0.95 &\approx Pr(\hat Y_{t+1} - 2 \sqrt{var(\hat Y_{t+1})} < Y_{t+1} < \hat Y_{t+1} - 2 \sqrt{var(\hat Y_{t+1})}) \\
0.95 &\approx Pr(0.026 - 2 \sqrt{0.0025} < Y_{t+1} < 0.026 + 2 \sqrt{var(0.0025}) \\
0.95 &\approx Pr(0.026 - 2 \sqrt{0.0025} < Y_{t+1} < 0.026 + 2 \sqrt{var(0.0025}) \\
0.95 &\approx Pr(-0.074 < Y_{t+1} < 0.126) \\
\end{aligned}
\end{equation}

Therefore we are 95% confident that the true value of $Y_{t+1}$ falls within the range $(-0.074, 0.126)$.

_4. Prove or disprove that the_ $MA(2)$ _above can be approximated by an_ $AR(\infty)$. _If that is the case, compute the infinite autoregressive weights_ $\pi_1$, $\pi_2$, _and_ $\pi_3$.

$Y_t$ is invertible if it has an infinite autoregressive representation, $AR(\infty)$, as

$$\pi(L) Y_t = \epsilon_t$$

where $\pi (L) = 1 - \pi_1 L - \pi_2 L^2 - ...$,

where $\pi_k -> 0$ as $n -> \infty$

An $MA(2)$ is invertible only if $\Theta(r^{-1}) = 0$ produces $|r| < 1$.

$Y_t = \Theta(L) \epsilon_t, \hspace{0.25cm} \Theta(L) = 1 + 0.5 L - 0.2 L^2$

Define $\Theta(r^{-1}) = 1 + 0.5r^{-1} - 0.2 r^{-2}$

$\Theta(r^{-1}) = 0 = r^2 + 0.5r^{1} - 0.2$

Then using the quadratic formula:

$r_{1,2} = \frac{-b \pm \sqrt{b^2 - 4ac}}{2a} = \frac{-0.5 \pm \sqrt{0.5^2 + 4(0.2)}}{2}$

$r_1 = 0.262347538$
$r_2 = -0.762347538$

Since $|r_1| < 1$ and $|r_2| < 1$, then MA is invertible, therefore it has an $AR(\infty)$ representation:

\begin{equation}\tilde \pi_k = -0.5 \tilde \pi_{k-1} + 0.2 \tilde \pi_{k-2} \end{equation}
\begin{equation}\tilde \pi_1 = -0.5 \tilde \pi_{0} + 0.2 \tilde \pi_{-1} =  -0.5 (1) + 0.2 (0) = -0.5\end{equation}
\begin{equation}\tilde \pi_2 = -0.5 \tilde \pi_{1} + 0.2 \tilde \pi_{0} =  -0.5 (-0.5) + 0.2 (1) = 0.45\end{equation} 
\begin{equation}\tilde \pi_3 = -0.5 \tilde \pi_{2} + 0.2 \tilde \pi_{1} =  -0.5 (0.45) + 0.2 (-0.5) = -0.325\end{equation}

Therefore

\begin{equation}- \tilde \pi_1 = \pi_1 = 0.5 \end{equation}
\begin{equation}- \tilde \pi_2 = \pi_2 = -0.45 \end{equation}
\begin{equation}- \tilde \pi_3 = \pi_3 = 0.325 \end{equation}

So, $Y_t = 0.5 Y_{t-1} - 0.45 Y_{t-2} + 0.325 Y_{t-3} + ...$

_5. Suppose you managed to estimate_ $\hat \sigma^2 = 0.0024$ _for the overfitted_ $MA(4)$:

$$Y_t = \epsilon_t + \theta_1 \epsilon_{t-1} + \theta_2 \epsilon_{t-2} + \theta_3 \epsilon_{t-3} + \theta_4 \epsilon_{t-4}$$.


_Test_ $H_0: Y_t \sim MA(2)$ _versus_ $H_1: Y_t \sim MA(4)$


The test for $H_0: Y_t \sim MA(2)$ _versus_ $H_1: Y_t \sim MA(4)$ is given by comparing the Likelihood Ratio Test statistics to the $\chi^2$ critical value with 2 degrees of freedom.

$$LRT_{stat} = n \cdot ln(\frac{\hat\sigma^2_R}{\hat \sigma^2}) = 150 \cdot ln(0.0025/0.0024) \approx 6.1233  \sim \chi_{0.95}^2(2)$$

The critical value for $\chi_{0.95}^2(2) \approx 5.991$ which is less than our statistic of $6.1233$ therefore we reject $H_0: \theta_3 = \theta_4 = 0$.

## 2 Practicum

### Problem 1
Use the seasonally adjusted series you worked with on Assignment 1 to perform the following tasks: Construct a measure of the business cycle $Y_t$ using the Trend Stationary approach and fit an $MA(q)$ process using both the $BIC(k)$ and $AIC(k)$ to estimate $q$. Include standard errors in brackets under your estimates of $\hat \theta_j$, and provide an estimate of $\sigma$. Perform a likelihood ratio test at the 5% level of 

$$H_0: Y_t \sim MA(q) \hspace{0.1cm} versus \hspace{0.1cm} H_1:Y_t \sim MA(q + 2)$$ 

where $q$ is chosen from the $BIC(k)$. Calculate the p-value of the test statistic.

The construction of the business cycle $Y_t$ comes from the residuals of the following model:

```{r 1a, message=FALSE, warning=FALSE, echo=FALSE}
### Data Load
library(xlsx)
library(dplyr)
CPE_Cons_Goods  <- read.xlsx("CONS_Canada.xls", sheetName = "Sheet3")
CPE_Cons_Goods <- CPE_Cons_Goods %>% mutate(X_t = log(Seasonally.Adjusted))
CPE_Cons_Goods <- CPE_Cons_Goods[complete.cases(CPE_Cons_Goods),]
model.CPE <- lm(X_t ~ seq_along(CPE_Cons_Goods$X_t) , data = CPE_Cons_Goods)
summary(model.CPE)

# Yt is the residuals of regression
Yt <- model.CPE$residuals

```

$$\underset{t}{\hat{X}_t} = \underset{1318.15}{\hat{\alpha}} + \underset{95.52}{\hat{\mu}}t = 12.07 + 0.008158$$

Next we constructed a table of $AIC(k)$ and $BIC(k)$ values for $k=0,1,...19$.
```{r 1b, echo=FALSE, eval=TRUE}
# q1
# function for AIC
AIC <- function(res, k, N){
  aic <-log(sum(res^2) / N)
  aic <- aic + 2 * k /N
  aic
}

BIC <- function(res, k, N){
  bic <-log(sum(res^2) / N)
  bic <- bic +  k *log(N)/N
  bic
}

# obtaining the aic and bic
aic.array <- rep(NA, 20)
bic.array <- rep(NA, 20)
N <- length(Yt)
for(ii in 0:19){
  model.arima <- arima(Yt, order = c(0, 0, ii), include.mean = FALSE)
  res.arima <- model.arima$residuals
  aic.array[ii + 1] <- AIC(res.arima, ii, N)
  bic.array[ii + 1] <- BIC(res.arima, ii, N)
}

```

```{r, echo=FALSE, message=FALSE, warning=FALSE}
library(knitr)
# Puttng AIC/BIC into a table
AIC_table <- data.frame(cbind(c(0:19),aic.array,bic.array))
rownames(AIC_table) <- c()
colnames(AIC_table) <- c("q", "AIC","BIC")
kable(round(AIC_table, digits=2))


print("Min AIC:")
which(aic.array == min(aic.array))-1
print("Min BIC:")
which(bic.array == min(bic.array))-1
```

The issue that arose however was that no matter how many values of q were added the BIC and AIC just kept chosing the next highest value of q. We were instructed by our TA to instead use a value of $q=5$ as the best model instead. The resulting model:

```{r, 1b2, echo=FALSE, message=FALSE, warning=FALSE}
model.ma5 <- arima(Yt, order = c(0, 0, 5), include.mean = FALSE)
model.ma5
```

$$\underset{(s.e.)}{\hat{Y}_t} = \epsilon_{t} + \underset{(0.0666)}{\hat{\theta_1}} \epsilon_{t-1} + \underset{(0.1136)}{\hat{\theta_2}} \epsilon_{t-2} + \underset{(0.1234)}{\hat{\theta_3}} \epsilon_{t-3} + \underset{(0.1026)}{\hat{\theta_4}} \epsilon_{t-4} + \underset{(0.0634)}{\hat{\theta_5}} \epsilon_{t-5} \\ = \epsilon_{t} + 1.6086 \epsilon_{t-1} + 1.9782 \epsilon_{t-2} + 1.9019 \epsilon_{t-3} + 1.1642 \epsilon_{t-4} + 0.5059 \epsilon_{t-5} $$

Finally we're asked to perform a likelihood ratio test at the 5% level of our $MA(5)$ model versus an $MA(7)$:

```{r 1c, echo=FALSE, message=FALSE, warning=FALSE}
model.ma7 <- arima(Yt, order = c(0, 0, 7), include.mean = FALSE)

LR <- N * log(model.ma5$sigma2 / model.ma7$sigma2)
hyp.test <- LR > qchisq(p = 0.95, df = 2) # if true reject H_0
# or use p-value:
pvalue.LR <- 1 - pchisq(q = LR, df = 2)

print("What is our test statistic?")
LR
print("Is our test statistic greater than our critical p-value?")
hyp.test
print("What is the p-value of our test statistic?")
pvalue.LR
```

### Problem 2

Use Box-Jenkins identification to identify and estimate an $ARMA(p,q)$ model for the Trend Stationary $Y_t$.

The way Box-Jenkins Identification works is by looking at the patterns in both our estimated autocorrelation function $\hat\rho(k)$ and our estimated partial autocorrelation function $\phi_{kk}$.  We can identify the most likely model by the following guide:

```{r 2aa, warning=FALSE, message=FALSE, echo=FALSE}
library(knitr)
library(kableExtra)
model <- c("AR(p)", "MA(q)", "ARMA(p,q)")
rho_k <- c("damped exponential", "cut-off at k=q", "damped exponential")
phi_kk <- c("cut-off at k=p", "damped exponential", "damped exponential")

kable( data.frame(Model = model, Rho_k = rho_k, Phi_kk = phi_kk))

```


We can use built in R functions ```acf(Yt)``` and ```pacf(Yt)``` to visualize these two patterns.


```{r 2a, eval=TRUE, echo=FALSE, fig.height=3.5}
# q2

# acf & pacf using R bulit-in function

acf(Yt) # plots autocorrelation function against k
pacf(Yt) # plots partial autocorrelation function against k

# can also use the method from previous assignments
# i.e acf & pacf using linear regression, not recommended.

## etc..

### We are choosing ARMA(p,q) here
# ACF damped-exponential, PACF is cut-off, so we get AR(p) 

```


What is immediately clear from our plots is that our Autocorrelation function is a damped exponential and our Partial Autocorrelation function has a cut-off.  This indicates that we have an $AR(p)$ model, from our graph, with the dotted lines indicating our Standard Error for our Partial ACF it is clear that the cutoff should be an AR(1). 

\pagebreak

### Problem 3

Using your result from (2), run the following diagnostic tests:

(a) Plot the standardized residuals $\hat z_t= \hat a_t/\hat\sigma$ of the fitted model and comment about whether the normal distribution appears to be appropriate.
(b) Run a formal Jarque-Bera test for normality.
(c) Run a Box-Pierce test for joint autocorrelation.
(d) Test for the presence of ARCH(6).

#### Fit of AR(1) Model from (2)
```{r 3, echo=FALSE, message=FALSE, warning=FALSE, eval=TRUE}
ar2 <- arima(Yt, order = c(1, 0, 0), include.mean=FALSE)
ar2
residuals <- ar2$residuals
```

$$\underset{(s.e.)}{\hat{Y}_t} = \underset{(0.0062)}{\hat{\phi_1}} Y_{t-1}  = 0.9946 Y_{t-1} $$

#### (a) Plot the standardized residuals
```{r 3a, warning=FALSE, message=FALSE, echo=FALSE, fig.height=3.5}
library(ggplot2)
sigma.hat.squared <- sum(ar2$residuals^2)/length(ar2$residuals)
sum(ar2$residuals^2)
length(ar2$residuals)
length(Yt)
mean(ar2$residuals)
sigma.hat <- sqrt(sigma.hat.squared)
Zt <- (residuals-mean(residuals))/sigma.hat
hist(Zt, breaks = 50, freq = F, main = "Zt", xlab = "Standardized Residuals")
curve(dnorm(x),col = "red", add = T)
```

From a quick visual inspection the Normal Distribution appears it might be appropriate for our fitted model.  However it is a little hard to tell exactly from visual inspection, there does appear to be something off with our fit.

\pagebreak

#### (b) Run a formal Jarque-Bera test for normality.

Our Jarque-Bera statistic is defined by $J_{stat} = n(\frac{\hat k_3^2}{6} + \frac{(\hat k_4-3)^2}{24}) \sim \chi_{0.95}^2(2)$

The Jarque-Bera statistic tests if our standardized residuals $z_t$ are in fact Normally Distributed.  Our null hypothesis $H_0:$ our residuals are normally distributed.

```{r 3b, echo=FALSE, message=FALSE, warning=FALSE}
  ####### Jarque Bera Test
# can fail this test in 2 ways:
# 1. normality is rejected completely
# 2. the model is normal but didn't pass JB test because of
# the existence of significant outliers or structure breaks

############################################################
# if an ARMA model passes all diagnostics except for JB due
# to outliers, it's still a good model
############################################################

# find standardized residuals (same as above)
Zt <- (residuals-mean(residuals))/sigma.hat
# calculate K_3, K_4, 
K_3 <- (1/length(Zt))*sum(Zt^3)
K_4 <- (1/length(Zt))*sum(Zt^4)
print("K_3")
K_3
print("K_4")
K_4
# find J_Stat and J_crit.
J_Stat <- length(Zt)*(((K_3^2)/6)+((K_4-3)^2)/24)
print("Jarque-Bera Statistic:")
J_Stat
J_Crit <- qchisq(0.95,2)
print("Jarque-Bera Critical Value:")
J_Crit

```

Our Jarque-Bera Statistic (6.99044) > $\chi_{0.95}^2(2)$ 5.991 so we reject our null hypothesis that our residuals are normally distributed.


#### (c) Run a Box-Pierce test for joint autocorrelation.

The Box-Pierce Test is to test whether or not there exists autocorrelation between $\epsilon_t$ and $\epsilon_{t-K}$ for any $k=1,...,M$ where $M = round_{up}(sqrt(N))$. Using the built in Box-Pierce test in R we have found a p-value of 0.000749:

```{r 3c, echo=FALSE, message=FALSE, warning=FALSE}

#### Box-Piece test
# use built-in function:
M <- sqrt(length(residuals))
M <- 14
M

Box.test(x = residuals, type = "Box-Pierce",lag= M) 
# returns p-value when run
```

Therefore we reject the null hypothesis that there does not exist correlation between some $\epsilon_t$ and $\epsilon_{t-K}$ any $k=1,...,14$.  Because there exists correlation between our residuals, we will move on to the ARCH test in part (d).


#### (d) Test for the presence of ARCH(6).

For our ARCH(6) test we are testing the null hypothesis that for our model:

$$\underset{(s.e.)}{\hat{Y}_t} = \underset{(0.0062)}{\hat{\phi_1}} Y_{t-1}  = 0.9946 Y_{t-1} + a_t$$

where 

$$a_t = z_t(\sigma^2 + \alpha_1 a_{t-1}^2 + ... + \alpha_6 a_{t-6}^2)^{1/2}$$
Our $H_0: \alpha_1 = \alpha_2 = \alpha_3 = \alpha_4 = \alpha_5 = \alpha_6 = 0$


```{r 3d, echo=FALSE, message=FALSE, warning=FALSE}

### ARCH(6) test
## again, use standardized residuals (name them std.residuals to use this code)
std.residuals <- Zt
N <- length(std.residuals)
std.res.sq <- std.residuals^2
ARCH.model <- lm(std.res.sq[-(1:6)]~std.res.sq[-c((1:5), N)] 
                     +  std.res.sq[-c(1:4,(N-1),N)] + std.res.sq[-c(1:3, (N-2):N)] 
                     + std.res.sq[-c(1:2, (N-3):N)] + std.res.sq[-c(1, (N-4):N)] 
                     + std.res.sq[-((N-5):N)] - 1)
R.squared <- summary(ARCH.model)$r.squared
ARCH.test.stat <- N * R.squared
ARCH.test.crit <- qchisq(0.95,6)
ARCH.Null.Hypothesis <- (ARCH.test.stat > ARCH.test.crit) # if true reject H_0
ARCH.test.stat
ARCH.test.crit
ARCH.Null.Hypothesis

```
As it turns out our ARCH Test Statistical is greater than our Critical value, so we reject the null hypothesis that $H_0: \alpha_1 = \alpha_2 = \alpha_3 = \alpha_4 = \alpha_5 = \alpha_6 = 0$.


## Appendix Code
```{r appendix, eval=FALSE}
### Data Load
library(xlsx)
library(dplyr)
CPE_Cons_Goods  <- read.xlsx("CONS_Canada.xls", sheetName = "Sheet3")
CPE_Cons_Goods <- CPE_Cons_Goods %>% mutate(X_t = log(Seasonally.Adjusted))
CPE_Cons_Goods <- CPE_Cons_Goods[complete.cases(CPE_Cons_Goods),]
model.CPE <- lm(X_t ~ seq_along(CPE_Cons_Goods$X_t) , data = CPE_Cons_Goods)
summary(model.CPE)

# Yt is the residuals of regression
Yt <- model.CPE$residuals

# q1
# function for AIC
AIC <- function(res, k, N){
  aic <-log(sum(res^2) / N)
  aic <- aic + 2 * k /N
  aic
}

BIC <- function(res, k, N){
  bic <-log(sum(res^2) / N)
  bic <- bic +  k *log(N)/N
  bic
}

# obtaining the aic and bic
aic.array <- rep(NA, 20)
bic.array <- rep(NA, 20)
N <- length(Yt)
for(ii in 0:19){
  model.arima <- arima(Yt, order = c(0, 0, ii), include.mean = FALSE)
  res.arima <- model.arima$residuals
  aic.array[ii + 1] <- AIC(res.arima, ii, N)
  bic.array[ii + 1] <- BIC(res.arima, ii, N)
}

# Puttng AIC/BIC into a table
AIC_table <- data.frame(cbind(c(0:19),aic.array,bic.array))
rownames(AIC_table) <- c()
colnames(AIC_table) <- c("q", "AIC","BIC")
round(AIC_table, digits=2)


print("Min AIC:")
which(aic.array == min(aic.array))-1
print("Min BIC:")
which(bic.array == min(bic.array))-1

model.ma5 <- arima(Yt, order = c(0, 0, 5), include.mean = FALSE)
model.ma5


model.ma7 <- arima(Yt, order = c(0, 0, 7), include.mean = FALSE)

LR <- N * log(model.ma5$sigma2 / model.ma7$sigma2)
hyp.test <- LR > qchisq(p = 0.95, df = 2) # if true reject H_0
# or use p-value:
pvalue.LR <- 1 - pchisq(q = LR, df = 2)

print("What is our test statistic?")
LR
print("Is our test statistic greater than our critical p-value?")
hyp.test
print("What is the p-value of our test statistic?")
pvalue.LR


model <- c("AR(p)", "MA(q)", "ARMA(p,q)")
rho_k <- c("damped exponential", "cut-off at k=q", "damped exponential")
phi_kk <- c("cut-off at k=p", "damped exponential", "damped exponential")

kable( data.frame(Model = model, Rho_k = rho_k, Phi_kk = phi_kk))


# q2

# acf & pacf using R bulit-in function

acf(Yt) # plots autocorrelation function against k
pacf(Yt) # plots partial autocorrelation function against k

# can also use the method from previous assignments
# i.e acf & pacf using linear regression, not recommended.

## etc..

### We are choosing ARMA(p,q) here
# ACF damped-exponential, PACF is cut-off, so we get AR(p) 



ar2 <- arima(Yt, order = c(1, 0, 0), include.mean=FALSE)
ar2
residuals <- ar2$residuals


library(ggplot2)
sigma.hat.squared <- sum(ar2$residuals^2)/length(ar2$residuals)
sigma.hat <- sqrt(sigma.hat.squared)
Zt <- residuals/sigma.hat
hist(Zt, breaks = 30, freq = F, main = "Zt", xlab = "Standardized Residuals")
curve(dnorm(x),col = "red", add = T)


  ####### Jarque Bera Test
# can fail this test in 2 ways:
# 1. normality is rejected completely
# 2. the model is normal but didn't pass JB test because of
# the existence of significant outliers or structure breaks

############################################################
# if an ARMA model passes all diagnostics except for JB due
# to outliers, it's still a good model
############################################################

# find standardized residuals (same as above)
Zt <- residuals/sigma.hat
# calculate K_3, K_4, 
K_3 <- (1/length(Zt))*sum(Zt^3)
K_4 <- (1/length(Zt))*sum(Zt^4)
print("K_3")
K_3
print("K_4")
K_4
# find J_Stat and J_crit.
J_Stat <- length(Zt)*(((K_3^2)/6)+((K_4-3)^2)/24)
print("Jarque-Bera Statistic:")
J_Stat
J_Crit <- qchisq(0.95,2)
print("Jarque-Bera Critical Value:")
J_Crit

#### Box-Piece test
# use built-in function:
M <- sqrt(length(residuals))
M <- 14
M

Box.test(x = residuals, type = "Box-Pierce",lag= M) 
# returns p-value when run

### ARCH(6) test
## again, use standardized residuals (name them std.residuals to use this code)
std.residuals <- Zt
N <- length(std.residuals)
std.res.sq <- std.residuals^2
ARCH.model <- lm(std.res.sq[-(1:6)]~std.res.sq[-c((1:5), N)] 
                     +  std.res.sq[-c(1:4,(N-1),N)] + std.res.sq[-c(1:3, (N-2):N)] 
                     + std.res.sq[-c(1:2, (N-3):N)] + std.res.sq[-c(1, (N-4):N)] 
                     + std.res.sq[-((N-5):N)] - 1)
R.squared <- summary(ARCH.model)$r.squared
ARCH.test.stat <- N * R.squared
ARCH.test.crit <- qchisq(0.95,6)
ARCH.Null.Hypothesis <- (ARCH.test.stat > ARCH.test.crit) # if true reject H_0
ARCH.Null.Hypothesis

```