---
title: "Time Series Analysis - Assignment 1"
author: "Christopher RIsi"
date: "May 9, 2018"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## 1 Theory

### Problem 1
Consider the model:

$$Y_t = \epsilon_t + \theta\epsilon^2_{t-1}, \hspace{0.25cm} where \hspace{0.25cm}\epsilon_t \hspace{0.1cm} \sim i.i.d. N(0, \sigma^2)$$
Compute the autocovariances $\gamma(k)$ of $Y_t$ for $k \geq 0$ and show whether the process is stationary.

In order for a process to be stationary we need its mean, variance, and covariances to **not** depend on time:

I.e. 

1) $E[Y_t] = E[Y_s] = \mu \hspace{0.25cm} \forall s,t$

2) $var(Y_t) = var(Y_s) = \sigma^2 \hspace{0.25cm} \forall s,t$

3) $cov(Y_t, Y_{t \pm k}) = cov(Y_s, Y_{s \pm k})$ for any k.
 

Proof of 1):
\begin{equation}
\begin{aligned}
&E[Y_t] = E[\epsilon_t] + \theta E[\epsilon^2_{t-1}] = 0 + \theta (Var(\epsilon_{t-1}) + E[\epsilon_{t-1}]^2) \\ 
&E[Y_t] = \theta \sigma^2 + 0 = \theta \sigma^2
\end{aligned}
\end{equation}

$\therefore E[Y_t]$ does not depend on t.

Proof of 2):
\begin{equation}
\begin{aligned}
&var(Y_t)= var(\epsilon_t + \theta \epsilon^2_{t-1})\\
&var(Y_t)= var(\epsilon_t) + \theta^2 var(\epsilon^2_{t-1}) + 2cov(\epsilon_t,\theta^2 \epsilon^2_{t-1}) \\
&var(Y_t)= \sigma^2 + \theta^2 var(\epsilon^2_{t-1}) + 0\\
&var(Y_t)= \sigma^2 + \theta^2 E[(\epsilon^2_{t-1} - E[\epsilon^2_{t-1}])^2]\\ 
&var(Y_t)= \sigma^2 + \theta^2 E[\epsilon^4_{t-1} - 2\epsilon^2_{t-1} E[\epsilon^2_{t-1}]+ E[\epsilon^2_{t-1}]^2]\\ 
&var(Y_t)= \sigma^2 + \theta^2 E[\epsilon^4_{t-1} - 2\epsilon^2_{t-1} \sigma^2+ \sigma^4]\\
&var(Y_t)= \sigma^2 + \theta^2 (E[\epsilon^4_{t-1}] - 2\sigma^2 E[\epsilon^2_{t-1}] + \sigma^4)\\
\end{aligned}
\end{equation}

$E[\epsilon^4_{t-1}]$ is the 4th central moment of the Normal Distribution which is equal to $3\sigma^4$.

\begin{equation}
\begin{aligned}
&var(Y_t)= \sigma^2 + \theta^2 (3\sigma^4 - 2\sigma^4 + \sigma^4) \\
&var(Y_t)= \sigma^2 + 2 \theta^2 \sigma^4 \\
\end{aligned}
\end{equation}

$\therefore var[Y_t]$ does not depend on t.

\pagebreak

Proof of 3):
\begin{equation}
\gamma(k) := cov(Y_t, Y_{t \pm k})
\end{equation}

_For k = 0_
\begin{equation}
\gamma(0) = cov(Y_t, Y_t) = var(Y_t) = \sigma^2 + 2 \theta^2 \sigma^4   
\end{equation}

_For k = 1_
\begin{equation}
\begin{aligned}
&\gamma(1)= E[(Y_t - E[Y_t])(Y_{t - 1} - E[Y_{t - 1}])] \\
&\gamma(1) = E[(Y_t -  \theta \sigma^2)(Y_{t- 1} -  \theta \sigma^2)] \\
&\gamma(1) = E[Y_t Y_{t - 1} -  \theta \sigma^2 Y_t - \theta \sigma^2 Y_{t- 1} +  \theta^2 \sigma^4] \\
&\gamma(1) = E[Y_t Y_{t - 1}] -  \theta \sigma^2 E[Y_t] - \theta \sigma^2 E[Y_{t- 1}] +  \theta^2 \sigma^4 \\
&\gamma(1) = E[Y_t Y_{t - 1}] -  \theta \sigma^2 (\theta \sigma^2) - \theta \sigma^2 (\theta \sigma^2) +  \theta^2 \sigma^4 \\
&\gamma(1) = E[Y_t Y_{t - 1}] -  \theta^2 \sigma^4 \\
&\gamma(1) = E[(\epsilon_t + \theta \epsilon^2_{t-1}) (\epsilon_{t- 1}+\theta\epsilon^2_{t - 2})] -  \theta^2 \sigma^4 \\
&\gamma(1) = E[\epsilon_t \epsilon_{t- 1}] + \theta E[\epsilon_{t} \epsilon^2_{t - 2}] +\theta E[\epsilon^3_{t-1} ] + \theta^2 E[\epsilon^2_{t-1} \epsilon^2_{t - 2}] -  \theta^2 \sigma^4 \\
\end{aligned}
\end{equation}

Note that the third central moment of a Normal Distribution is $E[\epsilon^3_{t-1}] = 0$

\begin{equation}
\begin{aligned}
&\gamma(1) = 0 + \theta (0) +\theta (0) + \theta^2 \sigma^4  -  \theta^2 \sigma^4 \\
&\gamma(1) = 0 
\end{aligned}
\end{equation}

_For k = 2 & k > 2_

Because $\epsilon_t$ is i.i.d. we will always have $E[\epsilon_t \epsilon_s] = 0 \hspace{0.25cm} \forall s,t$.

Following the pattern using brute force we will never have it so that $s = t$ for any further lags.  So we know:

$$\gamma(k) = - \theta^2 \sigma^4 \hspace{0.25cm} \forall \hspace{0.1cm} k \geq 2 $$
To be explicit
\begin{equation}
\begin{aligned}
&\gamma(2)= E[(Y_t - E[Y_t])(Y_{t - 2} - E[Y_{t - 2}])] \\
&\gamma(2) = E[(Y_t -  \theta \sigma^2)(Y_{t- 2} -  \theta \sigma^2)] \\
&\gamma(2) = E[Y_t Y_{t - 2} -  \theta \sigma^2 Y_t - \theta \sigma^2 Y_{t- 2} +  \theta^2 \sigma^4] \\
&\gamma(2) = E[Y_t Y_{t - 2}] -  \theta \sigma^2 E[Y_t] - \theta \sigma^2 E[Y_{t- 2}] +  \theta^2 \sigma^4 \\
&\gamma(2) = E[Y_t Y_{t - 2}] -  \theta \sigma^2 (\theta \sigma^2) - \theta \sigma^2 (\theta \sigma^2) +  \theta^2\sigma^4 \\
&\gamma(2) = E[Y_t Y_{t - 2}] -  \theta^2 \sigma^4 \\
&\gamma(2) = E[(\epsilon_t + \theta \epsilon^2_{t-1}) (\epsilon_{t - 2}+\theta\epsilon^2_{t - 3})] -  \theta^2 \sigma^4 \\
&\gamma(2) = E[\epsilon_t \epsilon_{t- 2}] + \theta E[\epsilon_{t} \epsilon^2_{t - 3}] +\theta E[\epsilon_{t-1}^2\epsilon_{t-2} ] + \theta^2 E[\epsilon^2_{t-1} \epsilon^2_{t - 3}] -  \theta^2 \sigma^4 \\
\end{aligned}
\end{equation}

For all pairs of $\epsilon$ inside of $E[.]$ we will never have $\epsilon_t \epsilon _s$ where $s = t$ so these terms are always 0 for all $k \geq 2$.

\begin{equation}
\begin{aligned}
\therefore &\gamma(o) = \sigma^2 + 2 \theta^2 \sigma^4 
&\gamma(k) = -  \theta^2 \sigma^4 \hspace{0.25cm} \forall \hspace{0.1cm} k \geq 1
\end{aligned}
\end{equation}

Our autocovariance function does not depend on t for all k.  This means our final condition for stationarity is satisfied.

### Problem 2
Consider the stationary $AR(1)$ representation:

$$Y_t = \phi Y_{t-1} + a_t, \hspace{0.25cm} a_t \sim i.i.d \hspace{0.1cm} N(0, \sigma^2)$$
Show that
$$Y_t  \sim N(0, \frac{\sigma^2}{1 - \phi^2} )$$
To start:

\begin{equation}
\begin{aligned}
&E[Y_t] = E[\phi Y_{t-1} + a_t] \\
&E[Y_t] = \phi E[Y_{t-1}] + E[a_t] \\
&E[Y_t] = \phi E[Y_{t-1}] + 0
\end{aligned}
\end{equation}

$Y_t$ is stationary so $E[Y_t] = E[Y_{t-1}]$

\begin{equation}
\begin{aligned}
&E[Y_t] = \phi E[Y_{t}] \\
(1 - \phi) &E[Y_t] = 0 \
\end{aligned}
\end{equation}

$\therefore$ as long as $\phi \ne 0$ then we know $E[Y_t] = 0$.  

Next:
\begin{equation}
\begin{aligned}
&var(Y_t) = var(\phi Y_{t-1} + a_t) \\
&var(Y_t) = \phi^2 var(Y_{t-1}) + var(a_t) + zero \hspace{0.1cm} cross \hspace{0.1cm}cov.
\end{aligned}
\end{equation}

Again $Y_t$ is stationary so $var(Y_{t-1}) = var(Y_{t})$ so,

\begin{equation}
\begin{aligned}
var(Y_{t}) &= \phi ^ 2 var[Y_t] + \sigma^2 \\
 (1- \phi^2)var(Y_{t}) &= \sigma^2 \\
 var(Y_{t}) &= \sigma^2 /(1- \phi^2)
\end{aligned}
\end{equation}

Also note that $var(Y_t) \ne \infty \therefore \phi \ne 1$ so $E[Y] = 0$

$$ \therefore Y_t  \sim N(0, \frac{\sigma^2}{1 - \phi^2} )$$
\pagebreak

### Problem 3

Consider a random walk model with a drift parameter $m$ expressed as 

$$Y_t = m + \phi Y_{t-1} + \epsilon_t$$

such that $E[\epsilon_t] = 0 \hspace{0.1cm} \forall \hspace{0.1cm} t$, $var[\epsilon_t] = \sigma^2 \hspace{0.1cm} \forall \hspace{0.1cm} t$, $cov[\epsilon_t, \epsilon_s] = 0 \hspace{0.1cm} for \hspace{0.1cm} t \ne s$ and $cov[\epsilon_t, Y_t] = 0 \forall t$.  Assume further that $|\phi| < 1$. Derive an expression for (1) $E[Y_t]$, (2) $var[Y_t]$, (3) the autocovariance function $\gamma(k)$, and (4) the autocorrelation function $\rho(k)$ of $Y_t$. 

_For $E[Y_t]$_ 

\begin{equation}
\begin{aligned}
Y_t &= m + \phi Y_{t-1} + \epsilon_t \\
Y_t &= m + \phi(m +\phi Y_{t-2} + \epsilon_{t-1}) + \epsilon_t \\
\end{aligned}
\end{equation}

Continue substituting and expand:

\begin{equation}
\begin{aligned}
Y_t &= m + \phi m + \phi^2 m + \phi^3 m + ... + \epsilon_t + \phi \epsilon_{t-1} + \phi^2 \epsilon_{t-2} + ...\\
Y_t &= \frac{m}{1-\phi} + \epsilon_t + \phi \epsilon_{t-1} + \phi^2 \epsilon_{t-2} + ...\\
\end{aligned}
\end{equation}

We can now take the expectations:

\begin{equation}
\begin{aligned}
E[Y_t] &= E[\frac{m}{1-\phi} + \epsilon_t + \phi \epsilon_{t-1} + \phi^2 \epsilon_{t-2} + ...]\\
E[Y_t] &= E[\frac{m}{1-\phi}] + E[\epsilon_t + \phi \epsilon_{t-1} + \phi^2 \epsilon_{t-2} + ...]\\
E[Y_t] &= E[\frac{m}{1-\phi}] + E[\epsilon_t] + E[\phi \epsilon_{t-1}] + E[\phi^2 \epsilon_{t-2}] + ...\\
E[Y_t] &= \frac{m}{1-\phi} + 0 + 0 + 0 + ...\\
E[Y_t] &= \frac{m}{1-\phi} \\
\end{aligned}
\end{equation}

_For $var[Y_t]$_ 

From (15) above:

\begin{equation}
\begin{aligned}
Y_t &= \frac{m}{1-\phi} + \epsilon_t + \phi \epsilon_{t-1} + \phi^2 \epsilon_{t-2} + ...\\
var(Y_t) &= var(\frac{m}{1-\phi} + \epsilon_t + \phi \epsilon_{t-1} + \phi^2 \epsilon_{t-2} + ...)\\
var(Y_t) &= var(\frac{m}{1-\phi}) + var(\epsilon_t) + var(\phi \epsilon_{t-1}) + var(\phi^2 \epsilon_{t-2}) + ... + zero \hspace{0.1cm} cross \hspace{0.1cm} cov.\\
var(Y_t) &= 0 + var(\epsilon_t) + \phi^2 var( \epsilon_{t-1}) + \phi^4 var( \epsilon_{t-2}) + ... \\
var(Y_t) &= \frac{\sigma^2}{1-\phi^2} \\
\end{aligned}
\end{equation}


\pagebreak

_For $\gamma(k)$_

\begin{equation}
\begin{aligned}
\gamma(k) &= E[(Y_t - \frac{m}{1-\phi})(Y_{t-k} - \frac{m}{1-\phi})] \\
\gamma(k) &= E[(Y_t Y_{t-k} - \frac{m}{1-\phi}Y_{t-k} - \frac{m}{1-\phi}Y_{t} + \frac{m^2}{(1-\phi)^2})] \\
\gamma(k) &= E[Y_t Y_{t-k}] - E[\frac{m}{1-\phi}Y_{t-k}] - E[\frac{m}{1-\phi}Y_{t}] + E[\frac{m^2}{(1-\phi)^2}]\\
\gamma(k) &= E[Y_t Y_{t-k}] - \frac{m}{1-\phi}E[Y_{t-k}] - \frac{m}{1-\phi}E[Y_{t}] + \frac{m^2}{(1-\phi)^2}\\
\gamma(k) &= E[Y_t Y_{t-k}] - \frac{m}{1-\phi}\frac{m}{1-\phi} - \frac{m}{1-\phi}\frac{m}{1-\phi} + \frac{m^2}{(1-\phi)^2}\\
\gamma(k) &= E[Y_t Y_{t-k}] - \frac{m^2}{(1-\phi)^2} \\
\gamma(k) &= E[(m + \phi Y_{t-1} + \epsilon_t) Y_{t-k}] - \frac{m^2}{(1-\phi)^2} \\
\gamma(k) &= E[m Y_{t-k} + \phi Y_{t-1}Y_{t-k} + \epsilon_t Y_{t-k}] - \frac{m^2}{(1-\phi)^2} \\
\gamma(k) &= m E[ Y_{t-k}] + E[\phi Y_{t-1}Y_{t-k}] + E[\epsilon_t Y_{t-k}] - \frac{m^2}{(1-\phi)^2} \\
\gamma(k) &= \frac{m^2}{1-\phi} + \phi E[ Y_{t-1}Y_{t-k}] + 0 - \frac{m^2}{(1-\phi)^2} \\
\gamma(k) &= \frac{m^2 (1-\phi)}{(1-\phi)^2} - \frac{m^2}{(1-\phi)^2} + \phi E[ Y_{t-1}Y_{t-k}]  \\
\gamma(k) &= \frac{- m^2 \phi}{(1-\phi)^2} + \phi E[ Y_{t-1}Y_{t-k}]  \\
\gamma(k) &= \phi (-\frac{ m^2 }{(1-\phi)^2} + E[ Y_{t-1}Y_{t-k}] ) \\
\gamma(k) &= \phi (-\frac{ m^2 }{(1-\phi)^2} + E[ Y_{t-1}Y_{t-k}] ) \\
\gamma(k) &= \phi \gamma(k-1) = \phi^2 \gamma(k-2) = \phi^3 \gamma(k-3) = \phi^4 \gamma(k-4) = ... = \phi^k \gamma(0)  \\
\gamma(k) &= \phi^k \frac{\sigma^2}{(1-\phi^2)}
\end{aligned}
\end{equation}

\pagebreak

_For $\rho(k)$_
By definition:
\begin{equation}
\begin{aligned}
\rho(k) &= \gamma(k) / \gamma(0)
\end{aligned}
\end{equation}

But we already know that:
\begin{equation}
\begin{aligned}
\gamma(k) &= \phi^k \gamma(0)  \\
\end{aligned}
\end{equation}

Therefore:

\begin{equation}
\begin{aligned}
\rho(k) &= \gamma(k) / \gamma(0) = \phi^k \gamma(0) / \gamma(0) = \phi^k \\
\rho(k) &= \phi^k \\
\end{aligned}
\end{equation}


\pagebreak

## 2 Practicum

### Problem 1 - For the *seasonally adjusted* version do the following:
#### (a) Linearize the raw time series; that is, obtain $X_t = ln (W_t)$ and present a graph of the resulting series $X_t$.
    
```{r a, message = FALSE, warning = FALSE}
library(xlsx)
library(dplyr)
library(ggplot2)
CPE_Cons_Goods  <- read.xlsx("CONS_Canada.xls", sheetName = "Sheet3")
CPE_Cons_Goods <- CPE_Cons_Goods %>% mutate(X_t = log(Seasonally.Adjusted))
CPE_Cons_Goods <- CPE_Cons_Goods[complete.cases(CPE_Cons_Goods),]

qplot(seq_along(CPE_Cons_Goods$X_t), CPE_Cons_Goods$X_t, geom = "line") + 
  xlab("Time") +
  ylab("X_t") +
  ggtitle("Adjusted Logarithmic Canadian Personal Expenditures on \n Consumer Goods and Services vs. Time")
```

\pagebreak

#### (b) TS approach: 
    
##### Regress $X_t$ on a constant and a time trend as $$X_t = \alpha + \mu * t + Y_t \hspace{0.5cm} with \hspace{0.25cm} Y_t \hspace{0.25cm} \sim \hspace{0.25cm} i.i.d. \hspace{0.25cm} N.(0, \sigma^2)$$

```{r b_1, message = FALSE, warning = FALSE}
model_b <- lm(X_t ~ seq_along(CPE_Cons_Goods$X_t) , data = CPE_Cons_Goods)
summary(model_b)
sum(resid(model_b)^2)
summary(model_b)$r.squared
```
    
$$\underset{t}{\hat{X}_t} = \underset{1318.15}{\hat{\alpha}} + \underset{95.52}{\hat{\mu}}t = 12.07 + 0.0082t$$
    
$n = 185  \hspace{0.5cm}$   $F-ratio = 9124  \hspace{0.5cm}$   $RSS = 0.704245  \hspace{0.5cm}$   $R^2 = 0.9803367$
    
```{r b_2, message = FALSE, warning = FALSE, echo = FALSE}
qplot(seq_along(CPE_Cons_Goods$X_t), CPE_Cons_Goods$X_t, geom = "line") + 
  geom_abline(intercept = model_b$coefficients[1], slope = model_b$coefficients[2], color = "red") +
  ggtitle("Logrithmic Seasonally Adjusted CDN Personal Expenditures \n on Consumer Goods & Services with Regression Fit") +
  xlab("Time") +
  ylab("X_t = ln(W_t)")
```
    
##### What is the annual growth rate of $W_t$? 
The annual growth rate of $\hat{W}_t$ is equal to $\hat{\mu}_t*4 \times 100\%$. So growth annually is 3.263%.

##### Use the Rule of 72 to determine how long it takes for $W_t$ to double
Using the *Rule of 72* it will take $W_t$, approximately `r round(72/3.263,2)` years to double.

\pagebreak

##### Present a graph of $Y_t$
Below we have various plots assessing the model fit.  The first plot shows the graph of $Y_t$
```{r b_3, message = FALSE, warning = FALSE, echo = FALSE}
qplot(seq_along(model_b$residuals), model_b$residuals) + 
    ylab("Residuals") + 
    xlab("Time") + 
    ggtitle("Graph of Y_t")
```
    
##### What is the sample mean of $Y_t$? 
The sample mean of $Y_t$ is given by:
```{r b_4, message = FALSE, warning = FALSE}
mean(model_b$residuals)
```    

##### How large is the largest percentage deviation of $W_t$ from its long-run trend?  When does this occur?
```{r b_5, message = FALSE, warning = FALSE, echo = TRUE}
CPE_Cons_Goods <- cbind(CPE_Cons_Goods[1:185,], model_b$residuals)
CPE_Cons_Goods$W_t_fitted_values <- exp(model_b$fitted.values)
CPE_Cons_Goods$W_t_Percent_Deviation <- abs(CPE_Cons_Goods$Seasonally.Adjusted - CPE_Cons_Goods$W_t_fitted_values)/CPE_Cons_Goods$W_t_fitted_values

max(CPE_Cons_Goods$W_t_Percent_Deviation)
max_dev_index <- which(CPE_Cons_Goods$W_t_Percent_Deviation == max(CPE_Cons_Goods$W_t_Percent_Deviation))
max_dev_index
CPE_Cons_Goods[max_dev_index,]
```
    
The largest percentage deviation from $W_t$ from its long-run trend is 14.397% and this occurs in our first observation which is the first quarter (Q1) of 1961.
    
##### Obtain the cycle from the previous regression and fit the following AR(1) model and estimate $\phi$ using ordinary least squares:
    
$$Y_t = \phi Y_{t-1} + a_t$$

```{r b_6, message = FALSE, warning = FALSE}
    Y_t <- model_b$residuals
    Y_t_minus1 <- lag(Y_t)
    resid <- data.frame(cbind(Y_t, Y_t_minus1))
    resid <- resid[-1,]
    model_AR_1 <- lm(Y_t ~ Y_t_minus1 - 1 , data = resid)
    summary(model_AR_1)
    sum(resid(model_AR_1)^2)
    summary(model_AR_1)$r.squared
```   

$$\underset{t}{\hat{Y}_t} = \underset{94.73}{\hat{\phi}} Y_{t-1} = 0.9754Y_{t-1}$$
$n = 184  \hspace{0.5cm}$   $F-ratio = 8973  \hspace{0.5cm}$   $RSS = 0.01366125  \hspace{0.5cm}$   $R^2 = 0.9800133$

$\hat{Y}_t$ is stationary *iff* $-1 < \phi < 1$, therefore we can assume our model is stationary because our estimate of $\hat{\phi} = 0.9754$ satisfies this condition.
    
#### (c) DS Approach: 
        
##### Present a graph of $\Delta X_t$. 
```{r c_1, message=FALSE, warning=FALSE, echo=FALSE}
CPE_Cons_Goods  <- read.xlsx("CONS_Canada.xls", sheetName = "Sheet3")
CPE_Cons_Goods <- CPE_Cons_Goods %>% mutate(X_t = log(Seasonally.Adjusted))
CPE_Cons_Goods <- CPE_Cons_Goods[complete.cases(CPE_Cons_Goods),]
CPE_Cons_Goods$DeltaX_t <- CPE_Cons_Goods$X_t - lag(CPE_Cons_Goods$X_t)

qplot(seq_along(CPE_Cons_Goods$DeltaX_t), CPE_Cons_Goods$DeltaX_t) + 
    ylab("Delta X_t") + 
    xlab("Time") + 
    ggtitle("Graph of Delta X_t")

```

##### Run the regression to obtain the Difference Stationary cycle as in $\Delta X_t = \mu + Y_t$

In an intercept only regression model, the intercept can be found by taking the mean of the response variable
```{r c_2, message=FALSE, warning=FALSE, echo=TRUE}
model_c <- lm(DeltaX_t ~ 1, data = CPE_Cons_Goods)
summary(model_c)
```

##### What is the annual growth rate of $W_t$?
The constant growth model is based on a quarterly differences so we need mu*4:
```{r c_3, message=FALSE, warning=FALSE, echo=TRUE}
mu <- model_c$coefficients[1]
mu*4
```
Therefore the annual growth rate is 3.54%

##### Use the Rule of 72 to determine how long it takes $W_t$ to double.
Using the *Rule of 72* it will take $W_t$, approximately `r round(72/3.542867,2)` years to double.

##### Present a graph of $Y_t$
```{r c_4, message=FALSE, warning=FALSE, echo=FALSE}
CPE_Cons_Goods$Y_t_DS <- c(NA,model_c$residuals)
qplot(seq_along(CPE_Cons_Goods$Y_t_DS), CPE_Cons_Goods$Y_t_DS) + 
    xlab("Time") + 
    ylab("Y_t for Difference Stationary ")  + 
    ggtitle("Graph of Y_t")

```

##### What is the sample mean of $Y_t$?
```{r c_5, message=FALSE, warning=FALSE, echo=TRUE}
mean(CPE_Cons_Goods$Y_t_DS, na.rm=TRUE)
```

##### What is the largest value of $Y_t$?  When did it occur?
```{r c_6, message=FALSE, warning=FALSE, echo=TRUE}
max_abs_Y_t <- max(abs(CPE_Cons_Goods$Y_t_DS),  na.rm=TRUE)
max_abs_Y_t
max_dev_index <- which((CPE_Cons_Goods$Y_t_DS == max_abs_Y_t) | (CPE_Cons_Goods$Y_t_DS == -max_abs_Y_t))
max_dev_index
CPE_Cons_Goods[max_dev_index,]
```
The largest value of $Y_t$ (assuming absolute difference from 0) is 0.03261828, this value occurs between the final quarter of 1990 and the first quarter of 1991.

##### Estimate $\phi$ in the AR(1) model $Y_t = \phi Y_{t-1} + a_t$ using ordinary least squares:
```{r c_7, message=FALSE, warning=FALSE, echo=TRUE}
Y_t <- model_c$residuals
Y_t_minus1 <- lag(Y_t)
resid <- data.frame(cbind(Y_t, Y_t_minus1))
resid <- resid[-1,]
model_AR_1 <- lm(Y_t ~ Y_t_minus1 - 1 , data = resid)
summary(model_AR_1)
sum(resid(model_AR_1)^2)
summary(model_AR_1)$r.squared
```

$\hat{Y}_t$ is stationary *iff* $-1 < \phi < 1$, therefore we can assume our model is stationary because our estimate of $\hat{\phi} = 0.005395$ satisfies this condition.

### Problem 2 - For the *seasonally unadjusted* version, do the following:
#### a)
##### Present a graph of $X_t = ln(W^S_t)$:
```{r 2_a_1, message=FALSE, warning=FALSE, echo=TRUE}
CPE_Cons_Goods <- CPE_Cons_Goods %>% mutate(X_t_S = log(Seasonally.Unadjusted))

qplot(seq_along(CPE_Cons_Goods$X_t_S), CPE_Cons_Goods$X_t_S, geom = "line") + 
  xlab("Time") +
  ylab("X_t") +
  ggtitle("Unajusted Logarithmic Canadian Personal Expenditures on \n Consumer Goods and Services vs. Time")

```

##### Run the regression to obtain the Trend Stationary Cycle $Y_t$:
```{r 2_a_2, message=FALSE, warning=FALSE, echo=TRUE}
CPE_Cons_Goods$d1 <- c(rep(c(1,0,0,0), times = length(CPE_Cons_Goods$X_t_S)/4),1)
CPE_Cons_Goods$d2 <- c(rep(c(0,1,0,0), times = length(CPE_Cons_Goods$X_t_S)/4),0)
CPE_Cons_Goods$d3 <- c(rep(c(0,0,1,0), times = length(CPE_Cons_Goods$X_t_S)/4),0)
CPE_Cons_Goods$d4 <- c(rep(c(0,0,0,1), times = length(CPE_Cons_Goods$X_t_S)/4),0)

model_2_a <- lm(X_t_S ~ d1 + d2 + d3 + d4 + seq_along(X_t_S) - 1, data = CPE_Cons_Goods)
summary(model_2_a)
```

##### Present a graph of $Y_t$:
```{r 2_a_3, message=FALSE, warning=FALSE, echo=TRUE}
qplot(seq_along(model_2_a$residuals), model_2_a$residuals) + 
    ylab("Residuals") + 
    xlab("Time") + 
    ggtitle("Graph of Y_t")
```

#### b)

##### Present a graph of $\Delta_4 X_t$:

```{r 2_b_1, message=FALSE, warning=FALSE, echo=TRUE}
CPE_Cons_Goods$XDelta4X_t_S <- CPE_Cons_Goods$X_t_S - lag(CPE_Cons_Goods$X_t_S, 4)
qplot(seq_along(CPE_Cons_Goods$XDelta4X_t_S), CPE_Cons_Goods$XDelta4X_t_S, geom ="line") + 
    ylab("Delta_4 X_t_S") + 
    xlab("Time") +
    ggtitle("Graph of Delta_4 X_t")
```


##### Run the regression to obtain the Difference Stationary cycle as in $\Delta_4 X_t = \mu + Y_t$
```{r 2_b_2, message=FALSE, warning=FALSE, echo=TRUE}
model_2b <- lm(XDelta4X_t_S ~ 1, data = CPE_Cons_Goods)
summary(model_2b)

```

##### What is the annual growth rate of $W_t$?
```{r 2_b_3, message=FALSE, warning=FALSE, echo=TRUE}
model_2b$coefficients

```
The annual growth rate of $W_T$ is 3.509%.

##### Use the Rule of 72 to determine how long it takes $W_t$ to double.
Using the *Rule of 72* it will take $W_t$, approximately `r round(72/3.508967,2)` years to double.

##### Present a graph of $Y_t$
```{r 2_b_4, message=FALSE, warning=FALSE, echo=TRUE}
qplot(seq_along(model_2b$residuals), model_2b$residuals) +
    ylab("Residuals") + 
    xlab("Time") + 
    ggtitle("Graph of Y_t")
```






